# analysis functions# import packages for simulation and calculationimport numpy as npfrom collections import Counterimport pickleimport osimport reimport copy as cpfrom brian2 import *from scipy.signal import welch, iirnotch, filtfilt################################ energy calculations ################################def E_RP_eq(V_Na, V_K, R_m_experimental, V_RP): # additionally alpha?    # input     # V_Na is the sodium reversal potential in V    # V_K is the potassium reversal potential in V    # R_m is the membrane resistance in Ohm    # V_RP is the resting potential in V        # output    # E_RP is the energy needed to keep the resting potential in ATP/s        V_h = -0.043 # in V     alpha = 0.05    e = 1.602e-19 # in C    A_R_m = 0.91    R_m = R_m_experimental * A_R_m    # sodium conductance (g_Na)    g_Na = (9 * alpha / R_m) * ((V_RP - V_h) / (9 * (1 + alpha) * (V_RP - V_h) + 12 * (V_K - V_RP) + 8 * alpha * (V_Na - V_RP)))    # potassium conductance (g_K)    g_K = (9 / R_m) * ((V_RP - V_h) / (9 * (1 + alpha) * (V_RP - V_h) + 12 * (V_K - V_RP) + 8 * alpha * (V_Na - V_RP)))    # h-channel conductance (g_h)    g_h = (4 / R_m) * ((3 * (V_K - V_RP) + 2 * alpha * (V_Na - V_RP)) / (9 * (1 + alpha) * (V_RP - V_h) + 12 * (V_K - V_RP) + 8 * alpha * (V_Na - V_RP)))    # resting energy    E_RP = (g_Na * (V_Na - V_RP) / 3 + g_h * (V_h - V_RP) / 4) / e    return E_RPdef E_AP_eq(r_post, A_overlap_AP, c_m, S_m, Delta_V):    # input     # r_post is the postsynaptic spiking frequency in Hz    # A_overlap_AP is the overlap factor between sodium and potassium currents    # C_m is the membrane capacitance in F    # Delta_V is the voltage difference between threshold voltage and action potential peak voltage in V        # output    # E_AP is the action potential energy in ATP/s        e = 1.602e-19 # in C    E_AP = r_post * A_overlap_AP * c_m * S_m * Delta_V / (3 * e) # E_AP = r_post * A_overlap_AP * C_m * Delta_V / (3 * e)    return E_APdef E_ST_eq(I_syn_e, A_overlap_syn):     # input     # I_syn_e is the current of excitatory synapses in A/s    # A_overlap_syn is the overlap factor between sodium and potassium currents of synapses        # output    # E_ST is energy needed for excitatory synaptic transmission (reversal of ion gradients) in ATP/s        e = 1.602e-19 # in C    E_ST = I_syn_e * A_overlap_syn / (3 * e)    return E_STdef E_glu_eq(r_post, N_boutons, E_glu_rec):    # input     # r_post is the postsynaptic spiking frequency in Hz    # N_boutons is the number of synaptic boutons    # E_glu_rec is the energy needed to recycle one vesicle of glutamate in ATP        # output    # E_glu is energy needed to recycle the vesicle of glutamate in ATP/s        p_ves = 0.6*np.exp(-r_post/5.0) # or simply 0.25 as Howarth 2012 or p_ves = 0.6*np.exp(-r_post/10.6) + 0.128 as Yu 2017    E_glu = r_post * N_boutons * p_ves * E_glu_rec     return E_gludef E_Ca_eq(r_post, N_boutons, E_Ca_ex):    # input     # r_post is the postsynaptic spiking frequency in Hz    # N_boutons is the number of synaptic boutons    # E_Ca_ex is the energy needed extrude the calcium of synaptic transmission in ATP        # output    # E_Ca is energy needed to extrude the calcium influx to release vesicles in synaptic transmission in ATP/s        p_ves = 0.6*np.exp(-r_post/5.0) # or simply 0.25 as Howarth 2012 or p_ves = 0.6*np.exp(-r_post/10.6) + 0.128 as Yu 2017    E_Ca = r_post * N_boutons * p_ves * E_Ca_ex    return E_Ca# total energy calculation functiondef E_tot_eq(R_m, C_m, E_L, r_post, I_syn_e, V_thresh):     # input     # R_m is the membrane resistance in MOhm    # C_m is the membrane capacitance in pF -> not used because only somatic capacitance measured but     # E_L is the resting or leak potential in mV    # r_post is the mean postsynaptic firing rate    # I_syn_e is an array of excitatory synaptic current in nA/s    # V_thresh is the threshold voltage in mV        # output     # E_tot, E_HK, E_RP, E_AP, E_ST, E_glu, E_Ca_ex are the energy consumers of the postsynaptic neuron in ATP/s    V_Na = 0.05 # in V    V_K = -0.1 # in V    #R_m = 100000000 # in Ohm    #V_RP = -0.07 # in V        A_overlap_AP = 1.24 # 1.24 Howarth 2012 refering to Carter and Bean 2009 # 1.5 Yu referring to Hallermann 2012        d_axon = 3e-7 # in m    l_axon = 4e-2 # in m * 1.89 for human     S_axon = np.pi * d_axon * l_axon # surface of a cylinder    d_soma = 2.5e-5 # in m    S_soma = np.pi * d_soma**2 # surface of a sphere    d_dendrite = 3*d_axon # in m    l_dendrite = l_axon/9 # in m    S_dendrite = np.pi * d_dendrite * l_dendrite # surface of a cylinder    S_m = S_axon + S_soma + S_dendrite        c_m = 1e-2 # in F/m^2        V_peak = 25 # in mV    Delta_V = (V_peak - V_thresh)*10**(-3) # in ΔV = 0.05 V    N_boutons = 8000 # for human *1.89    #E_postsyn = 1.64e5 # in ATP    A_overlap_syn = 1.1 # derived overlap factor, accounts for inhibitory cost being scaled similarly    E_glu_rec = 1.468e4 # in ATP vs 1.07E+04 in Howarth 2012    E_Ca_ex = 1.2e4 # in ATP    #r_pre_spike = 1 # in Hz    #E_prepostsynions = 1.64e5 # in ATP        # energy contributors with unit transformation    E_HK = 5.38e8 # in ATP/s    E_RP = E_RP_eq(V_Na, V_K, R_m*10**(6), E_L*10**(-3))    E_AP = E_AP_eq(r_post, A_overlap_AP, c_m, S_m, Delta_V) # if C_m from measurement: E_AP_eq(r_post, A_overlap_AP, C_m*10**(-12), Delta_V) #    E_ST = E_ST_eq(np.mean(I_syn_e)*10**(-9), A_overlap_syn) #E_ST = E_ST_eq_experimental(f_pre_spike, N_boutons, E_prepostsynions)    E_glu = E_glu_eq(r_post, N_boutons, E_glu_rec)    E_Ca = E_Ca_eq(r_post, N_boutons, E_Ca_ex)    E_tot = E_RP + E_AP + E_ST + E_glu + E_Ca + E_HK    return E_tot, E_HK, E_RP, E_AP, E_ST, E_glu, E_Cadef calculate_E_tot_contributors(R_m, C_m, E_L, V_thresh, r_post, I_syn_e):    # calculate energy contributors for different r_post & I_syn_e values        # input     # R_m is the membrane resistance in MOhm    # C_m is the membrane capacitance in pF    # E_L is the resting or leak potential in mV    # V_thresh is the threshold voltage in mV    # r_post is a list of the mean postsynaptic firing rates    # I_syn_e is a list of mean excitatory synaptic current in nA/s    # output    # E_tot_list, E_HK_list, E_RP_list, E_AP_list, E_ST_list, E_glu_list, E_Ca_list are lists of the corresponding contributor to the energy         E_tot_list, E_HK_list, E_RP_list, E_AP_list, E_ST_list, E_glu_list, E_Ca_list = [], [], [], [], [], [], []    # calculate & save corresponding energy values    for r_post, I_syn_e in zip(r_post, I_syn_e):        E_tot, E_HK, E_RP, E_AP, E_ST, E_glu, E_Ca = E_tot_eq(R_m, C_m, E_L, r_post, I_syn_e, V_thresh)                E_tot_list.append(E_tot)        E_HK_list.append(E_HK)        E_RP_list.append(E_RP)        E_AP_list.append(E_AP)        E_ST_list.append(E_ST)        E_glu_list.append(E_glu)        E_Ca_list.append(E_Ca)    return E_tot_list, E_HK_list, E_RP_list, E_AP_list, E_ST_list, E_glu_list, E_Ca_listdef calculate_energy_contribution_percentages(E_tot, E_HK, E_RP, E_AP, E_ST, E_glu, E_Ca):    # translate absolute energy contributions into percentages of E_tot        # input     # E_tot, E_HK, E_RP, E_AP, E_ST, E_glu, E_Ca are the energy contributors in ATP/s    # E_HK_percent, E_RP_percent, E_AP_percent, E_ST_percent, E_glu_percent, E_Ca_ex_percent are the energy contributors in percent of E_tot        # convert to numpy arrays for easier element-wise operations    E_tot = np.array(E_tot)        # calculate the contribution of each energy component (as a percentage of total energy)    E_HK_percent = (np.array(E_HK) / np.array(E_tot)) * 100    E_RP_percent = (np.array(E_RP) / np.array(E_tot)) * 100    E_AP_percent = (np.array(E_AP) / np.array(E_tot)) * 100    E_ST_percent = (np.array(E_ST) / np.array(E_tot)) * 100    E_glu_percent = (np.array(E_glu) / np.array(E_tot)) * 100    E_Ca_percent = (np.array(E_Ca) / np.array(E_tot)) * 100    return E_HK_percent, E_RP_percent, E_AP_percent, E_ST_percent, E_glu_percent, E_Ca_percentdef single_energy_calculation_experimental_data_Zeldenrust(R_m, C_m, E_L, V_thresh, r_post, I_inj, T):     # calculate mutual information of experimental Zeldenrust data        # input    # R_m is the membrane resistance in MOhm    # C_m is the membrane capacitance in pF    # E_L is the leak potential in mV    # V_thresh is the firing threshold in mV    # r_post is the neuronal firing rate in Hz    # I_inj is an array of injected current in nA    # T is the duration of the simulation in ms        # output    # E_tot is the total energy used    # energy_results is a dictionary of energy contributors        E_tot, E_HK, E_RP, E_AP, E_ST, E_glu, E_Ca = E_tot_eq(R_m, C_m, E_L, r_post, I_inj, V_thresh)    energy_results = {        "E_tot": E_tot,        "E_HK": E_HK,        "E_RP": E_RP,        "E_AP": E_AP,        "E_ST": E_ST,        "E_glu": E_glu,        "E_Ca": E_Ca}    return E_tot/1e9, energy_results################################ information calculation functions ################################def calculate_CV_ISI(spike_times_post):     # calculate ISI statistics         # input    # spike_times_post is a list of postsynaptic spike times        # output    # CV_ISI is the CVs based on ISIs    # convert spike_times to an array for convenience    spike_times_post_array = np.array(spike_times_post)    # Calculate ISIs from spike times    ISIs = np.diff(spike_times_post_array)    # Calculate mean and std ISI    mean_ISI = np.mean(ISIs) # mean_ISI is the mean ISI of the spike train    std_ISI = np.std(ISIs) # std_ISI is the standard deviation of ISIs of the spike train        CV_ISI = std_ISI / mean_ISI        return CV_ISIdef calculate_CV_V_m(V_m, V_thresh):     # calculate V_m statistics         # input    # V_m is the time-dependent subthreshold membrane voltage in mV    # V_thresh is the threshold voltage in mV        # output    # CV_V_m is the CV of the subthreshold membrane voltage normed by distance to threshold        CV_V_m = - np.std(V_m) / (np.mean(V_m) - V_thresh)        return CV_V_mdef get_signal_spike_times(spike_times_e, N_e_noise):    # bin spike train in time        # input    # spike_times_e is an array of the excitatory spike trains    # N_e_noise is the number excitatory noise synapses        # output    # signal_spike_times_e is a dictionary of the excitatory signalling spike trains        signal_spike_times_e = {}    counter = 0    for key, value in spike_times_e.items():        counter += 1        if counter >= N_e_noise:            signal_spike_times_e[key] = value    return signal_spike_times_edef time_binning(spike_times, T, bin_time):    # bin spike train in time        # input    # spike_times is a list of spike times    # T is the duration of the simulation in ms    # bin_time is the binsize in time in ms        # output    # time_binned_spike_times is an array of the time binned spike train        # define the binning array    bins_time = np.arange(0, T, bin_time)    time_binned_spike_times, _ = np.histogram(spike_times, bins = bins_time)    return time_binned_spike_timesdef rate_binning(time_binned_spike_times, bins_rate):    # bin spike train rates        # input    # time_binned_spike_times is an array of time binned data    # bins_rate is the number of bins of the rate binning        # output    # rate_time_binned_spike_times is an array of a rate binned and time binned spike train        # define the binning array    bins_rate_arr = np.linspace(0, max(time_binned_spike_times), bins_rate + 1)    # assign the values of time_binned_spike_times to the indices of the corresponding bins_rate_arr    time_binned_spike_times_digitized = np.digitize(time_binned_spike_times, bins_rate_arr) - 1  # subtract 1 to make bins zero-indexed        # assign digitized time binned spike times to rate binning    rate_time_binned_spike_times = bins_rate_arr[time_binned_spike_times_digitized]    return rate_time_binned_spike_timesdef time_bin_rate_bin_spike_trains(spike_times_post, spike_times_e, w_e, bin_time, bins_rate, T):    # bin presynaptic & postsynaptic spike train in time and rate space         # input    # spike_times_post are the postsynaptic spike times in ms    # spike_times_e are the presynaptic spike times in s    # w_e is an array of synaptic weights    # bin_time is the binsize in time in ms    # bins_rate is the number of bins of the rate binning    # T is the duration of the simulation in ms        # output    # rate_time_binned_spike_times_pre is an array of the time and rate binned weighted presynaptic spike trains    # rate_time_binned_spike_times_post is an array of the time and rate binned postsynaptic spike trains        # time and rate bin pre spike trains (in seconds)    time_binned_spike_times_pre = [time_binning(spike_times_pre, T/1000, bin_time/1000) for spike_times_pre in spike_times_e.values()]    weighted_time_binned_spike_times_pre = w_e[:, np.newaxis]*time_binned_spike_times_pre # weight the time binned presynaptic spike trains with the corresponding weight    flattened_weighted_time_binned_spike_times_pre = np.sum(weighted_time_binned_spike_times_pre, axis=0) # flatten the pre-synaptic binned spike trains to a single array    rate_time_binned_spike_times_pre = rate_binning(flattened_weighted_time_binned_spike_times_pre, bins_rate)        # time and rate bin post spike train (in ms)    time_binned_spike_times_post = time_binning(spike_times_post, T, bin_time)    rate_time_binned_spike_times_post = rate_binning(time_binned_spike_times_post, bins_rate)    return rate_time_binned_spike_times_pre, rate_time_binned_spike_times_postdef joint_probability(train_1, train_2):    # calculate joint probability distribution        # input    # train_1, train_2 are the given trains to calculate the joint probability        # output    # joint probability of the trains        assert len(train_1) == len(train_2), 'The length of both trains must be equal.'    joint_freq = Counter(zip(train_1, train_2)) # count the frequency of pairs (train1[i], train2[i])    total_count = len(train_1) # total number of pairs    joint_prob = {key: value / total_count for key, value in joint_freq.items()} # convert joint frequency to joint probability    return joint_probdef individual_probability(train):    # calculate individual probability distribution        # input    # train is the given trains to calculate the individual probability        # output    # individual probability of train        total_count = len(train) # total number of entries    individual_freq = Counter(train)    individual_prob = {key: value / total_count for key, value in individual_freq.items()}    return individual_probdef mutual_information(stimulus, neuronal_response):    # calculate mutual information        # input    # stimulus is an array of the weighted rate and time binned presynaptic input trains    # neuronal_response is an array of the rate and time binned postsynaptic output train        # output    # MI is the mutual information between stimulus and neuronal response in bits        joint_prob = joint_probability(stimulus, neuronal_response)    stimulus_prob = individual_probability(stimulus)    neuronal_response_prob = individual_probability(neuronal_response)        MI = 0    for (s, r), p_sr in joint_prob.items():        p_s = stimulus_prob[s]        p_r = neuronal_response_prob[r]        MI += p_sr * np.log2(p_sr / (p_s * p_r))    return MIdef transfer_entropy(X, Y, delay):    # calculate transfer entropy        # input    # X is an array of the weighted rate and time binned presynaptic input trains    # Y is an array of the rate and time binned postsynaptic output train    # delay is the delay of the transfer entropy        # output    # TE is the transfer entropy between stimulus and neuronal response with delay in bits        assert len(X) == len(Y), 'The length of both time series must be equal.'    # calculate joint and conditional probabilities    joint_prob_xyz = joint_probability(X[delay:], list(zip(Y[:-delay], X[:-delay])))    joint_prob_xy = joint_probability(X[delay:], Y[:-delay])    joint_prob_xx = joint_probability(X[delay:], X[:-delay])    prob_x = individual_probability(X[delay:])    TE = 0    for (x, (y, x_delay)), p_xyz in joint_prob_xyz.items():        p_xy = joint_prob_xy[(x, y)]        p_xx_delay = joint_prob_xx[(x, x_delay)]        p_x = prob_x[x]                arg1 = p_xy * p_xx_delay        arg2 = p_xyz * p_x        # corrections avoiding log(0)        if arg1 == 0.0: arg1 = 1e-8        if arg2 == 0.0: arg2 = 1e-8        TE += p_xyz * np.log2(arg2 / arg1)    return TEdef get_optimal_bin_sizes(spike_times_post, spike_times_e, w_e, T, bin_time_range, bins_rate_range):     # test different bin sizes (time & rate space)        # input    # spike_times_post is a list of postsynaptic spike times in ms    # spike_times_pre is a list of lists of presynaptic spike times in ms    # w_e are the excitatory weights    # T is the duration of the simulation in ms    # bin_time_range is an array of binsizes in time in ms    # bins_rate_range is an array of binsizes in frequency space    # binning_results is a list of different binning results        binning_results = [] # save results in list    # 2D grid search for optimal time and rate bin sizes/numbers    for bin_time in bin_time_range:        for bins_rate in bins_rate_range:            # get binned trains            rate_time_binned_spike_times_pre_bin, rate_time_binned_spike_times_post_bin = time_bin_rate_bin_spike_trains(spike_times_post, spike_times_e, w_e, int(bin_time), int(bins_rate), T)            # calculate MI and TE for bin size/number combination            mi_bin = mutual_information(rate_time_binned_spike_times_pre_bin, rate_time_binned_spike_times_post_bin)            te_bin = transfer_entropy(rate_time_binned_spike_times_pre_bin, rate_time_binned_spike_times_post_bin, delay=1)            binning_results.append((int(bin_time), int(bins_rate), mi_bin, te_bin)) # save results    return binning_resultsdef calculate_tuning_curve(spike_times_post):    # calculate tuning curves        # input    # spike_times_post is an array of the postsynaptic spike times in ms        # output    # tuning_curve is a list of rates for respective orientations in Hz        # define stimulus presentation windows    stimulus_windows = [(3000, 4500),(6000, 7500),(9000, 10500),(12000, 13500),(15000, 16500),(18000, 19500),(21000, 22500)]    # count spikes within each stimulus window    spike_counts = []    for start, end in stimulus_windows:        count = np.sum((spike_times_post >= start) & (spike_times_post < end))        spike_counts.append(count)    # convert list to array and divide by 1.5s, the stimulus presentation time to scale result to Hz    tuning_curve = np.array(spike_counts)/1.5 # to scale it to Hz    return tuning_curvedef calculate_OSI(tuning_curve):    # calculate OSI        # input    # tuning_curve is a list of rates for respective orientations in Hz        # output    # OSI is the orientation selectivity index         OSI = 0        r_preferred = tuning_curve[3]    r_nonpreferred = np.mean([tuning_curve[0],tuning_curve[-1]])    #OSI = (r_preferred-r_nonpreferred)/(r_preferred+r_nonpreferred)    if r_preferred > 0:         OSI = (r_preferred-r_nonpreferred)/(r_preferred+r_nonpreferred)            return OSIdef calculate_CV_ISI_tuning_curve(spike_times_post):    # calculates the CV of ISIs for each stimulus window        # input    # spike_times_post is a list of postsynaptic spike times        # output    # CV_ISI_tuning_curve is a list of the CVs based on ISIs for the 7 stimulus presentations        stimulus_windows = [(3000, 4500),(6000, 7500),(9000, 10500),(12000, 13500),(15000, 16500),(18000, 19500),(21000, 22500)]        spike_times_post_array = np.array(spike_times_post)    CV_ISI_tuning_curve = []    for start, end in stimulus_windows:        # extract spikes in current window        spikes_in_window = spike_times_post_array[(spike_times_post_array >= start) & (spike_times_post_array < end)]        # check number of spikes        if len(spikes_in_window) < 3:            CV_ISI_tuning_curve.append(0)        if len(spikes_in_window) >= 3:            ISIs = np.diff(spikes_in_window) # compute ISIs            CV_ISI = np.std(ISIs) / np.mean(ISIs)            CV_ISI_tuning_curve.append(CV_ISI)    return CV_ISI_tuning_curvedef calculate_mutual_information_tuning_curve(tuning_curve):    # calculate mutual information between hidden state (stimulus presenation) & tuning curve        # input    # tuning_curve is a list of rates for respective orientations in Hz        # output    # MI_tuning_curve is the mutual information between hidden state input and neuronal response in bits    # define hidden state    hidden_state = [0, 1, 2, 3, 2, 1, 0]        hidden_state = np.asarray(hidden_state)    tuning_curve = np.asarray(tuning_curve)        # calculate mutual information between hidden state & postsynaptic response during stimulus presentation    MI_tuning_curve = mutual_information(hidden_state, tuning_curve)    #print(str(MI_tuning_curve) + ' bits')    #pf.plot_tuning_curves(hidden_state, tuning_curve, 'stimulus', 'neuronal response', normalized=True, color_CTR = 'blue', color_FR = 'orange', savename=None)    return MI_tuning_curvedef calculate_mutual_information_post(spike_times_list, stimulus_windows):    # calculate mutual information based on firing rates of during hidden states        # input     # spike_times_list is a list of spike times (one per trial) in ms    # stimulus_windows is a list of 7 tuples indicating stimulus onset and offset times in ms        # output    # MI_post is the mutual information in bits based on firing rates of during hidden states        if spike_times_list is None or len(spike_times_list) == 0:        return 0.0        spike_counts_all_trials=[] # initialize list to store spike counts (i.e., firing rates) during the 7 stimulus windows for each trial    for trial_spikes in spike_times_list: # loop over each trial's spike times.        trial_counts = np.zeros((1, 7), dtype=np.float32) # create a zero-filled array to store spike counts for the 7 stimulus windows                     for idx, (start, end) in enumerate(stimulus_windows): # loop over the 7 stimulus windows            trial_counts[0, idx] = np.sum((trial_spikes >= start) & (trial_spikes <= end)) # for each spike time, check if it falls within the current window & increment the spike count if it does        spike_counts_all_trials.append(trial_counts) # store the spike count vector (length 7) for this trial    spike_counts_array = np.concatenate(spike_counts_all_trials, axis=0).astype(np.int64) # create a 2D array of all trials' 7-window spike counts (n_trials x 7) & convert to integer    max_count = np.max(spike_counts_array) # find the maximum spike count across all trials and windows for binning    # prepare a matrix to store joint distribution of 4 rows of hidden states (0-3 because 4-6 are shown twice) & spike_count columns (0 to max_rate)    joint_counts = np.zeros((4, max_count + 1), dtype=int) # joint matrix: rows = 4 hidden states, columns = spike count values     # build the joint histogram by assuming the first 4 windows correspond to states 0–3 & the last 3 windows (4–6) are shown twice (window 4 → state 2, 5 → state 1, 6 → state 0) --> encoding collapses the 7 windows into 4 states by summing    for win_idx in range(7):        state_idx = win_idx if win_idx <= 3 else 6 - win_idx        joint_counts[state_idx, :] += np.bincount(spike_counts_array[:, win_idx], minlength=max_count + 1)    # compute marginals    rate_marginals = np.sum(joint_counts, axis=0) # sum over rows = spike count distribution    state_marginals = np.sum(joint_counts, axis=1) # sum over columns = hidden state distribution    total_count = np.sum(joint_counts) # total number of entries = normalization constant    # compute MI    MI_post=0    for state in range(4):        for rate in range(max_count + 1):            joint = joint_counts[state, rate]            if joint > 0:                p_joint = joint / total_count                p_state = state_marginals[state] / total_count                p_rate = rate_marginals[rate] / total_count                MI_post += p_joint * np.log2(p_joint / (p_state * p_rate))            return MI_postdef update_E_tot(results):    # divide E_tot by 10^9 in the results dictionary    # input    # results is the dictionary of results    # output    # results is the updated dictionary of results        for key, res in results.items():        E_tot = res.get('E_tot', None)        res['E_tot'] = E_tot / 1e9    return results    def update_mutual_information_post(results):    # compute mutual information based on spike_times_post of several grid runs for each grid point in the results dictionary    # input    # results is the dictionary of results    # output    # results is the updated dictionary of results        # define stimulus windows    stimulus_windows = [(3000, 4500), (6000, 7500), (9000, 10500), (12000, 13500), (15000, 16500), (18000, 19500), (21000, 22500)]        count = 1    for key in results:        # load results data        spike_times_post = results[key].get('spike_times_post', None)        r_post = results[key].get('r_post', None)        E_tot = results[key].get('E_tot', None)                print(f'{count}/{len(results)}')        count+=1        # filter out non-spiking trials (alternative: non-spiking trials = empty array)        if r_post == 0.0:            MI_post, MI_post_per_energy, MICE_post, MICE_post_per_energy = 0.0, 0.0, 0.0, 0.0        # compute MI for spiking trials        if r_post > 0.0:            MI_post = calculate_mutual_information_post(spike_times_post, stimulus_windows)            MI_post_per_energy = MI_post/E_tot*10**9            MICE_post = MI_post/r_post            MICE_post_per_energy = MI_post/r_post/E_tot*10**9        results[key]['MI_post'] = MI_post        results[key]['MI_post_per_energy'] = MI_post_per_energy        results[key]['MICE_post'] = MICE_post        results[key]['MICE_post_per_energy'] = MICE_post_per_energy    return resultsdef update_mutual_information_binsize(results, spike_times_e, N_e_noise, w_e, T, bin_time_mi=1200, bins_rate_mi=150, bin_time_te=782, bins_rate_te=39):    # update mutual information & transfer entropy based on (updated) optimal binsizes    # input    # results is the dictionary of results    # output    # results is the updated dictionary of results        # define stimulus windows    #stimulus_windows = [(3000, 4500), (6000, 7500), (9000, 10500), (12000, 13500), (15000, 16500), (18000, 19500), (21000, 22500)]        # bin presynaptic spike train    spike_times_e_signal = get_signal_spike_times(spike_times_e, N_e_noise)    w_e_signal = w_e[N_e_noise-1:]        # bin  presynaptic signaling input for mutual information    time_binned_spike_times_pre_mi = [time_binning(spike_times_pre, T/1000, bin_time_mi/1000) for spike_times_pre in spike_times_e_signal.values()]    weighted_time_binned_spike_times_pre_mi = w_e_signal[:, np.newaxis]*time_binned_spike_times_pre_mi # weight the time binned presynaptic spike trains with the corresponding weight    flattened_weighted_time_binned_spike_times_pre_mi = np.sum(weighted_time_binned_spike_times_pre_mi, axis=0) # flatten the pre-synaptic binned spike trains to a single array    rate_time_binned_spike_times_pre_mi = rate_binning(flattened_weighted_time_binned_spike_times_pre_mi, bins_rate_mi)    # bin  presynaptic signaling input for transfer entropy    time_binned_spike_times_pre_te = [time_binning(spike_times_pre, T/1000, bin_time_te/1000) for spike_times_pre in spike_times_e_signal.values()]    weighted_time_binned_spike_times_pre_te = w_e_signal[:, np.newaxis]*time_binned_spike_times_pre_te # weight the time binned presynaptic spike trains with the corresponding weight    flattened_weighted_time_binned_spike_times_pre_te = np.sum(weighted_time_binned_spike_times_pre_te, axis=0) # flatten the pre-synaptic binned spike trains to a single array    rate_time_binned_spike_times_pre_te = rate_binning(flattened_weighted_time_binned_spike_times_pre_te, bins_rate_te)    count=1    for key in results:        # load results data        spike_times_post_list = results[key].get('spike_times_post', None)        r_post = results[key].get('r_post', None)        E_tot = results[key].get('E_tot', None)                print(f'{count}/{len(results)}')        count+=1                # initialize results        MI_new_list, MI_new_per_energy_list, MICE_new_list, MICE_new_per_energy_list, TE_new_list, TE_new_per_energy_list, TECE_new_list, TECE_new_per_energy_list = [], [], [], [], [], [], [], []                for spike_times_post in spike_times_post_list:                                    # filter out non-spiking trials (alternative: non-spiking trials = empty array)            if r_post == 0.0:                MI_new_list, MI_new_per_energy_list, MICE_new_list, MICE_new_per_energy_list = [0.0], [0.0], [0.0], [0.0]                TE_new_list, TE_new_per_energy_list, TECE_new_list, TECE_new_per_energy_list = [0.0], [0.0], [0.0], [0.0]                # compute MI for spiking trials            if r_post > 0.0:                                # calculate mutual information between presynaptic input & postsynaptic output                #rate_time_binned_spike_times_pre_mi_new, rate_time_binned_spike_times_post_mi_new = time_bin_rate_bin_spike_trains(spike_times_post, spike_times_e_signal, w_e, bin_time_mi, bins_rate_mi, T)                time_binned_spike_times_post_mi = time_binning(spike_times_post, T, bin_time_mi)                rate_time_binned_spike_times_post_mi = rate_binning(time_binned_spike_times_post_mi, bins_rate_mi)                MI_new = mutual_information(rate_time_binned_spike_times_pre_mi, rate_time_binned_spike_times_post_mi)                MI_new_per_energy = MI_new/E_tot*10**9                MICE_new = MI_new/r_post                MICE_new_per_energy = MI_new/r_post/E_tot*10**9                                MI_new_list.append(MI_new)                 MI_new_per_energy_list.append(MI_new_per_energy)                 MICE_new_list.append(MICE_new)                 MICE_new_per_energy_list.append(MICE_new_per_energy)                                 #rate_time_binned_spike_times_pre_te_new, rate_time_binned_spike_times_post_te_new = time_bin_rate_bin_spike_trains(spike_times_post, spike_times_e_signal, w_e_scaled[N_e_noise-1:], bin_time_te, bins_rate_te, T)                time_binned_spike_times_post_te = time_binning(spike_times_post, T, bin_time_te)                rate_time_binned_spike_times_post_te = rate_binning(time_binned_spike_times_post_te, bins_rate_te)                TE_new = transfer_entropy(rate_time_binned_spike_times_pre_te, rate_time_binned_spike_times_post_te, delay = 1)                TE_new_per_energy = TE_new/E_tot*10**9                TECE_new = TE_new/r_post                TECE_new_per_energy = TE_new/r_post/E_tot*10**9                                TE_new_list.append(TE_new)                 TE_new_per_energy_list.append(TE_new_per_energy)                 TECE_new_list.append(TECE_new)                 TECE_new_per_energy_list.append(TECE_new_per_energy)                         print(f'{np.mean(MI_new_list)} bits')        print(f'{np.mean(TE_new_list)} bits')        results[key]['MI_new'] = np.mean(MI_new_list)        results[key]['MI_new_per_energy'] = np.mean(MI_new_per_energy_list)        results[key]['MICE_new'] = np.mean(MICE_new_list)        results[key]['MICE_new_per_energy'] = np.mean(MICE_new_per_energy_list)        results[key]['TE_new'] = np.mean(TE_new_list)        results[key]['TE_new_per_energy'] = np.mean(TE_new_per_energy_list)        results[key]['TECE_new'] = np.mean(TECE_new_list)        results[key]['TECE_new_per_energy'] = np.mean(TECE_new_per_energy_list)    return results################################ general analysis functions ################################def complete_analysis(spike_times_post, V_m, T, R_m, C_m, E_L, I_syn_e, V_thresh, spike_times_e, w_e_scaled, N_e_noise):    # perform complete analysis        # input    # spike_times_post is a list of postsynaptic spike times in ms    # V_m is the time-dependent subthreshold membrane voltage in mV    # T is the duration of the simulation in ms    # R_m is the membrane resistance in MOhm    # C_m is the membrane capacitance in pF    # E_L is the resting potential (leak reversal potential) in mV    # I_syn_e is an array of length n of excitatory synaptic current    # V_thresh is the threshold voltage in mV    # spike_times_e is a dictionary where keys are neuron indices and values are arrays of spike times for excitatory neurons    # w_e_scaled is an array of length N_e with scaled excitatory synaptic weights    # N_e_noise is the number excitatory noise synapses        # output    # r_post is the postsynaptic firing rate in Hz    # E_tot is the total energy in ATP/s    # CV_V_m is the CV of the subthreshold membrane voltage normed by distance to threshold    # tuning_curve is a list of rates for respective orientations in Hz    # OSI is the orientation selectivity index     # OSI_per_energy is the orientation selectivity index per energy in 1/(10^9 ATP/s)    # CV_ISI_tuning_curve is a list of the CVs based on ISIs for the 7 stimulus presentations    # CV_ISI_tuning_curve_per_energy is a list of the CVs based on ISIs for the 7 stimulus presentations per energy in 1/(10^9 ATP/s)    # MI_tuning_curve is the mutual information between hidden state input and neuronal response in bits    # MI_tuning_curve_per_energy is the mutual information between hidden state input and neuronal response per energy in bits/(10^9 ATP/s)    # MICE_tuning_curve is the mutual information per spike in bits    # MICE_tuning_curve_per_energy is the mutual information per spike per energy in bits/(10^9 ATP/s)    # CV_ISI is the CVs based on ISIs    # CV_ISI_per_energy CV_ISI is the CVs based on ISIs per energy in 1/(10^9 ATP/s)    # MI is the mutual information between stimulus and neuronal response in bits    # MI_per_energy is the mutual information between stimulus and neuronal response per energy in bits/(10^9 ATP/s)    # TE is the transfer entropy between stimulus and neuronal response with delay in bits    # TE_per_energy is the transfer entropy between stimulus and neuronal response with delay per energy in bits/(10^9 ATP/s)    # MICE is the mutual information per spike in bits    # MICE_per_energy is the mutual information per spike per energy in bits/(10^9 ATP/s)    # TECE is the transfer entropy per spike in bits    # TECE_per_energy per energy in bits/(10^9 ATP/s)    # E_vec is a tuple containing the total energy and all energy contributions            # calculate CV and MI and TE (per energy)    if len(spike_times_post) < 3:        # calculate energy and firing frequency         r_post = 0 # None        E_tot, E_HK, E_RP, E_AP, E_ST, E_glu, E_Ca = E_tot_eq(R_m, C_m, E_L, r_post, I_syn_e, V_thresh)        # variability of the substhreshold membrane voltage        CV_V_m = calculate_CV_V_m(V_m, V_thresh)        # information measures        tuning_curve = [0,0,0,0,0,0,0] # None        OSI, OSI_per_energy, CV_ISI_tuning_curve, CV_ISI_tuning_curve_per_energy, MI_tuning_curve, MI_tuning_curve_per_energy, MICE_tuning_curve, MICE_tuning_curve_per_energy, CV_ISI, CV_ISI_per_energy, MI, MI_per_energy, TE, TE_per_energy, MICE, MICE_per_energy, TECE, TECE_per_energy = 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 # None, None, None, None, None, None, None, None, None, None, None, None            if len(spike_times_post) >= 3:        # calculate energy and firing frequency         r_post = len(spike_times_post) / T * 1000 # is the postsynaptic firing rate in Hz        E_tot, E_HK, E_RP, E_AP, E_ST, E_glu, E_Ca = E_tot_eq(R_m, C_m, E_L, r_post, I_syn_e, V_thresh)                # variability of the substhreshold membrane voltage        CV_V_m = calculate_CV_V_m(V_m, V_thresh)                # information measures        # calculate calculate tuning curve        tuning_curve = calculate_tuning_curve(spike_times_post)                # calcualte OSI        OSI = calculate_OSI(tuning_curve)        OSI_per_energy = OSI/E_tot*10**9                # calculate coefficient of variation        CV_ISI_tuning_curve = calculate_CV_ISI_tuning_curve(spike_times_post)        CV_ISI_tuning_curve_per_energy = np.array(CV_ISI_tuning_curve) / E_tot *10**9        # calculate mutual information between hidden state (stimulus presentation) & postsynaptic output        MI_tuning_curve = calculate_mutual_information_tuning_curve(tuning_curve)        MI_tuning_curve_per_energy = MI_tuning_curve/E_tot*10**9        MICE_tuning_curve = MI_tuning_curve / r_post        MICE_tuning_curve_per_energy = MI_tuning_curve_per_energy / r_post            # calculate coefficient of variation        CV_ISI = calculate_CV_ISI(spike_times_post)        CV_ISI_per_energy = CV_ISI / E_tot *10**9                # calculate mutual information between presynaptic input & postsynaptic output        spike_times_e_signal = get_signal_spike_times(spike_times_e, N_e_noise)                bin_time_mi = 1200 # in ms        bins_rate_mi = 150        rate_time_binned_spike_times_pre_mi, rate_time_binned_spike_times_post_mi = time_bin_rate_bin_spike_trains(spike_times_post, spike_times_e_signal, w_e_scaled[N_e_noise-1:], bin_time_mi, bins_rate_mi, T)        MI = mutual_information(rate_time_binned_spike_times_pre_mi, rate_time_binned_spike_times_post_mi)        MI_per_energy = MI/E_tot*10**9        MICE = MI / r_post        MICE_per_energy = MI_per_energy / r_post        # calculate transfer entropy        bin_time_te = 294 # in ms        bins_rate_te = 58         rate_time_binned_spike_times_pre_te, rate_time_binned_spike_times_post_te = time_bin_rate_bin_spike_trains(spike_times_post, spike_times_e_signal, w_e_scaled[N_e_noise-1:], bin_time_te, bins_rate_te, T)        TE = transfer_entropy(rate_time_binned_spike_times_pre_te, rate_time_binned_spike_times_post_te, delay = 1)        TE_per_energy = TE/E_tot*10**9        TECE = TE / r_post        TECE_per_energy = TE_per_energy / r_post        E_vec = (E_tot, E_HK, E_RP, E_AP, E_ST, E_glu, E_Ca)        #plot_template_two_graph(np.linspace(0,T/1000,len(rate_time_binned_spike_times_pre_mi)), rate_time_binned_spike_times_pre_mi/max(rate_time_binned_spike_times_pre_mi), np.linspace(0,T/1000,len(rate_time_binned_spike_times_post_mi)), rate_time_binned_spike_times_post_mi/max(rate_time_binned_spike_times_post_mi), 'time / s', 'normalized firing rate', 'normalized rate and time binned weighted presynaptic input', 'normalized rate and time binned neuronal response', 'presynaptic input and postsynaptic output for MI calculation', savename=None)    #plot_template_two_graph(np.linspace(0,T/1000,len(rate_time_binned_spike_times_pre_te)), rate_time_binned_spike_times_pre_te/max(rate_time_binned_spike_times_pre_te), np.linspace(0,T/1000,len(rate_time_binned_spike_times_post_te)), rate_time_binned_spike_times_post_te/max(rate_time_binned_spike_times_post_te), 'time / s', 'normalized firing rate', 'normalized rate and time binned weighted presynaptic input', 'normalized rate and time binned neuronal response', 'presynaptic input and postsynaptic output for TE calculation', savename=None)    #plot_template_one_graph([-90,-60,-30,0,30,60,90], tuning_curve/max(tuning_curve), 'Deviation from preferred orientation / °', 'Mean firing rate / Hz', 'Tuning curve', savename=None)        return r_post, E_tot, CV_V_m, tuning_curve, OSI, OSI_per_energy, CV_ISI_tuning_curve, CV_ISI_tuning_curve_per_energy, MI_tuning_curve, MI_tuning_curve_per_energy, MICE_tuning_curve, MICE_tuning_curve_per_energy, CV_ISI, CV_ISI_per_energy, MI, MI_per_energy, TE, TE_per_energy, MICE, MICE_per_energy, TECE, TECE_per_energy, E_vecdef print_complete_analysis(r_post, E_tot, CV_V_m, tuning_curve, OSI, OSI_per_energy, CV_ISI_tuning_curve, CV_ISI_tuning_curve_per_energy, MI_tuning_curve, MI_tuning_curve_per_energy, MICE_tuning_curve, MICE_tuning_curve_per_energy, CV_ISI, CV_ISI_per_energy, MI, MI_per_energy, TE, TE_per_energy, MICE, MICE_per_energy, TECE, TECE_per_energy, E_vec):    # print results of analysis        # input    # r_post is the postsynaptic firing rate in Hz    # E_tot is the total energy in ATP/s    # CV_V_m is the CV of the subthreshold membrane voltage normed by distance to threshold    # tuning_curve is a list of rates for respective orientations in Hz    # OSI is the orientation selectivity index     # OSI_per_energy is the orientation selectivity index per energy in 1/(10^9 ATP/s)    # CV_ISI_tuning_curve is a list of the CVs based on ISIs for the 7 stimulus presentations    # CV_ISI_tuning_curve_per_energy is a list of the CVs based on ISIs for the 7 stimulus presentations per energy in 1/(10^9 ATP/s)    # MI_tuning_curve is the mutual information between hidden state input and neuronal response in bits    # MI_tuning_curve_per_energy is the mutual information between hidden state input and neuronal response per energy in bits/(10^9 ATP/s)    # MICE_tuning_curve is the mutual information per spike in bits    # MICE_tuning_curve_per_energy is the mutual information per spike per energy in bits/(10^9 ATP/s)    # CV_ISI is the CVs based on ISIs    # CV_ISI_per_energy CV_ISI is the CVs based on ISIs per energy in 1/(10^9 ATP/s)    # MI is the mutual information between stimulus and neuronal response in bits    # MI_per_energy is the mutual information between stimulus and neuronal response per energy in bits/(10^9 ATP/s)    # TE is the transfer entropy between stimulus and neuronal response with delay in bits    # TE_per_energy is the transfer entropy between stimulus and neuronal response with delay per energy in bits/(10^9 ATP/s)    # MICE is the mutual information per spike in bits    # MICE_per_energy is the mutual information per spike per energy in bits/(10^9 ATP/s)    # TECE is the transfer entropy per spike in bits    # TECE_per_energy per energy in bits/(10^9 ATP/s)    # E_vec is a tuple containing the total energy and all energy contributions        E_HK = E_vec[1]    E_RP = E_vec[2]    E_AP = E_vec[3]    E_ST = E_vec[4]     E_glu = E_vec[5]     E_Ca = E_vec[6]         print('Total energy:', f"{E_tot:.2e}", 'ATP/s')    print('House keeping energy:', f"{E_HK:.2e}", 'ATP/s')    print('Resting energy:', f"{E_RP:.2e}", 'ATP/s')    print('AP energy:', f"{E_AP:.2e}", 'ATP/s')    print('Synaptic transmission energy:', f"{E_ST:.2e}", 'ATP/s')    print('Glutamate recycling energy:', f"{E_glu:.2e}", 'ATP/s')    print('Ca2+ extrusion energy:', f"{E_Ca:.2e}", 'ATP/s')    print(f'Variability of subthreshold membrane voltage: {round(CV_V_m,2)}')    if r_post > 0:        print('Postsynaptic firing rate:', f"{round(r_post,2)}", 'Hz')        # tuning-curve based metrics        print('Tuning curve spike counts:', tuning_curve)        print(f'OSI: {round(OSI,2)}')        print(f'OSI per energy: {round(OSI_per_energy,2)}')        print(f'CV_ISI (based on tuning_curve): {["{:.2f}".format(x) for x in CV_ISI_tuning_curve]}')        print(f'CV_ISI (based on tuning_curve) per energy: {["{:.2f}".format(x) for x in CV_ISI_tuning_curve_per_energy]} 1/(10^9 ATP/s)')        print(f'Mutual information (based on tuning_curve): {round(MI_tuning_curve,2)} bits')        print(f'Mutual information (based on tuning_curve) per energy: {round(MI_tuning_curve_per_energy,2)} bits/(10^9 ATP/s)')        print(f'Coding efficiency (based on MI_tuning_curve): {round(MICE_tuning_curve,2)} bits/AP')        print(f'Coding efficiency (based on MI_tuning_curve) per energy: {round(MICE_tuning_curve_per_energy,2)} bits/(AP * 10^9 ATP/s)')        # rate-based metrics        print(f'CV_ISI: {round(CV_ISI,2)}')        print(f'CV_ISI per energy: {round(CV_ISI_per_energy,2)} 1/(10^9 ATP/s)')        print(f'Mutual information (MI): {round(MI,2)} bits')        print(f'Mutual information (MI) per energy: {round(MI_per_energy,2)} bits/(10^9 ATP/s)')        print(f'Transfer entropy (TE): {round(TE,2)}')        print(f'Transfer entropy (TE) per energy: {round(TE_per_energy,2)} bits/(10^9 ATP/s)')        print(f'Coding efficiency (based on MI): {round(MICE,2)} bits/AP')        print(f'Coding efficiency (based on MI) per energy: {round(MICE_per_energy,2)} bits/(AP * 10^9 ATP/s)')        print(f'Coding efficiency (based on TE): {round(TECE,2)} bits/AP')        print(f'Coding efficiency (based on TE) per energy: {round(TECE_per_energy,2)} bits/(AP * 10^9 ATP/s)')        def print_value_for_key(results, specific_key, value_name):    # print a specific value for a given key from the results dictionary        # input    # results is a dictionary of all results    # specific_key is a string of the specific key    # value_name is a string of the desired value name    if specific_key in results:        if value_name in results[specific_key]:            print(f'"{value_name}" for:  "{specific_key}": {results[specific_key][value_name]}')            return results[specific_key][value_name]        else:            print(f'Value name "{value_name}" does not exist in the dictionary for key "{specific_key}".')    else:        print(f'Key "{specific_key}" does not exist in the results dictionary.')def print_all_values(results, value_name):    # print all values for a specific value name in the results dictionary        # input    # results is a dictionary of all results    # value_name is a string of the desired value name        print(f'All values for "{value_name}":')    for key, values in results.items():        if value_name in values:            print(f'Key "{key}": {values[value_name]}')        else:            print(f'Key "{key}": Value name "{value_name}" does not exist in the dictionary.')            def binarize_spike_train(T, spike_times):    # binarize a spike train        # input    # T is the duration of the simulation in ms    # spike_times is a list of spiketimes        # output    # spikes_binary_post is an array of binary representations of the spike train        dt = 1    spikes_binary_post = np.zeros(int(T/dt)) # initialize binary spike train    for t_spike in spike_times:         spikes_binary_post[int(t_spike/dt)] = 1 # if spiketime occurs in spike_times set to 1 (spike)    return spikes_binary_post################################ data loading functions ################################def load_data(savedname):    # input    # savedname is the saved name of .pkl file    # output    # results_single_run is a dictionary of the results        with open('../Data/' + str(savedname) + '.pkl', 'rb') as f: # load mean data         results_data = pickle.load(f) # load data     return results_datadef load_data_grid(savedname, model_mode):    # input    # savedname is the name of the saved grid as .pkl file    # model_mode is a string & decides which model is used for simulation        # output    # results_mean, results_std, results_error, results_normalized_error, results_number_data_points are dictionaries of loaded data     # saving structure is {savename}_{run_number}_results_LIF    name = str(savedname) + '_results_' + str(model_mode)        results_mean = load_data(name + '_mean') # load mean data     results_std = load_data(name + '_std') # load standard deviation data     results_error = load_data(name + '_error') # load standard error data     results_normalized_error = load_data(name + '_normalized_error') # load normalized standard error data    results_number_data_points = load_data(name + '_number_data_points') # load number of data points data         return results_mean, results_std, results_error, results_normalized_error, results_number_data_points################################ grid data analysis functions ################################def is_valid(v):    # function which tests if value should be included or excluded    # input    # v is a value (integer or string or boolean)        # output    # boolean value         if v is None: # exclude Nones        return False    if isinstance(v, (int, float)):        return v != 0  # exclude 0s    if isinstance(v, (list, np.ndarray)):  # exclude 0 lists (tuning curves)        v_array = np.array(v)        return not np.all(v_array == 0)    return Truedef load_and_process_multiple_results(savedname, model_mode, folder_path, results_mode='mean', mean_over_zeros=True, spike_times_mode=False):    # load result grids of single runs & perform results_mode action    # input    # savedname is the saved name of .pkl files    # model_mode is a string deciding which model is used    # optional folder_path if script is not in folder of files    # results_mode is the mode to perform the analysis    # mean_over_zeros is an optional argument which excludes all 0 values before meaning    # spike_times_mode collect all spike times during meaning or not    # output    # processed_results is a dictionary of processed results    # list to hold all result dictionaries    all_results = []    # file names to process    pattern_results = re.compile(fr'{savedname}_(\d+)_results_{model_mode}\.pkl')    # iterate through all files in the directory & collect relevant & matching files    for file_name in os.listdir(folder_path):        match = pattern_results.match(file_name)        if match:            file_path = os.path.join(folder_path, file_name)            with open(file_path, 'rb') as f:                results = pickle.load(f)                all_results.append(results)    if not all_results:        raise ValueError("No matching files found in the folder.")    # initialize a dictionary to store the processed results    processed_results = {}    # list of keys to skip    exception_keys = ['spike_times_post']    # process each key from the first result set as a template    template_keys = all_results[0].keys()    for key in template_keys:        # initialize a sub-dictionary for this key        processed_results[key] = {}        # process each sub-key for the conditions        sub_keys = all_results[0][key].keys()        for sub_key in sub_keys:            if sub_key in exception_keys:                if spike_times_mode is True and results_mode == 'mean':                    all_spike_times_post = []                    for result in all_results:                        spike_times_post = result[key][sub_key]                        all_spike_times_post.append(spike_times_post)                    processed_results[key][sub_key] = all_spike_times_post                continue            elif sub_key in exception_keys and spike_times_mode is False:                continue                        else:                # ensure we are averaging only non-zero and non-None results ([0, 0, 0, 0, 0, 0, 0] for tuning curves)                try:                    values = [result[key][sub_key] for result in all_results if is_valid(result[key][sub_key])]                    #values = [result[key][sub_key] for result in all_results if result[key][sub_key] not in [None, 0, [0, 0, 0, 0, 0, 0, 0]]]                    number_valid_values = len(values)                                        # always store the number of data points, regardless of len(values)                    if results_mode == 'number_data_points':                        processed_results[key][sub_key] = number_valid_values                    # For other options, check if there are more than 3 valid data points                    elif number_valid_values > 5: # change "threshold" for better statistics                        if results_mode == 'mean':                            processed_results[key][sub_key] = np.mean(values, axis=0)                        elif results_mode == 'std':                            processed_results[key][sub_key] = np.std(values, axis=0)                        elif results_mode == 'error':                            processed_results[key][sub_key] = np.std(values, axis=0) / np.sqrt(number_valid_values)                        elif results_mode == 'normalized_error':                            mean_value = np.mean(values, axis=0)                            processed_results[key][sub_key] = np.std(values, axis=0) / np.sqrt(number_valid_values) / mean_value                    else:                        processed_results[key][sub_key] = 0  # handle cases where not enough data points exist                except TypeError as e:                    print(f"Skipping sub-key '{sub_key}' for key '{key}' due to incompatible data type: {e}")    return processed_resultsdef weight_results_by_spiking_trials(results_mean, results_number_data_points):    # correct meaning mechanism by multiplying with percentage of spiking trials        # input    # dictionary of meaned results & dictionary of number of spiking trials per key    # output    # dictionary of weighted results    combined_results = {} # initialize new dictionary    max_number_data_points = max(max(nested_dict.values()) for nested_dict in results_number_data_points.values()) # get number of meaned data points        exception_keys = ['spike_times_post']  # do not weight these keys        for key in results_mean: # iterate through keys        combined_results[key] = {} # create new dictionary for key-values pairs        for subkey in results_mean[key]:            if subkey in exception_keys: # pass through without weighting                combined_results[key][subkey] = results_mean[key][subkey]            else: # apply weighting                combined_results[key][subkey] = results_mean[key][subkey] * results_number_data_points[key][subkey] / max_number_data_points                 return combined_results################################ trajectory functions ################################def find_closest_grid_point_to_coordinate(results, coordinate):    # find closest grid point to given coordinate    # input     # results is the dictionary of the simulation data    # coordinate is a tuple of the (R_m, E_L, w_scale) coordinate    # output    # closest_grid_point_coordinate is the coordinate tuple of the closest grid point to coordinate    # closest_key is the key string of the closest grid point to coordinate        grid_points = []    for key in results.keys():        parts = key.split("_")        R_m = float(parts[1])        E_L = float(parts[3])        w_scale = float(parts[5])        grid_points.append((R_m, E_L, w_scale))    grid_points = np.array(grid_points)    # convert input coordinate to numpy array    coordinate = np.array(coordinate)    # compute distances to all grid points    distances = np.linalg.norm(grid_points - coordinate, axis=1)    # find the index of the closest grid point    closest_idx = np.argmin(distances)    closest_grid_point_coordinate = tuple(grid_points[closest_idx])    # build the corresponding key    closest_key = f"Rm_{closest_grid_point_coordinate[0]}_EL_{closest_grid_point_coordinate[1]}_wscale_{closest_grid_point_coordinate[2]}"    return closest_grid_point_coordinate, closest_keydef extract_ranges_from_keys(results):    # extract parameter ranges from keys         # input    # results is the dictionary of the simulation data    # output    # R_m_range, E_L_range, w_scale_range are arrays of the parameter values used for simulation    # initialize list of used R_m, E_L, w_scale    R_m_set = set()    E_L_set = set()    w_scale_set = set()    for key in results:        parts = key.split('_')        R_m_set.add(float(parts[1]))        E_L_set.add(float(parts[3]))        w_scale_set.add(float(parts[5]))        # sort the unique values to get the range for each parameter    R_m_range = sorted(R_m_set)    E_L_range = sorted(E_L_set)  # descending order?    w_scale_range = sorted(w_scale_set)    return R_m_range, E_L_range, w_scale_rangedef find_neighbors(results, coordinate):    # find the 26 (3x3x3 - 1) neighbouring coordinates        # input    # results is the dictionary of the simulation data    # coordinate is a tuple of the (R_m, E_L, w_scale) coordinate    # output    # neighbor_keys is a list of neightbouring keys    R_m, E_L, w_scale = coordinate         R_m_range, E_L_range, w_scale_range = extract_ranges_from_keys(results)    dR_m = np.abs(np.abs(R_m_range[1]) - np.abs(R_m_range[0]))    dE_L = np.abs(np.abs(E_L_range[1]) - np.abs(E_L_range[0]))    dw_scale = np.abs(np.abs(w_scale_range[1]) - np.abs(w_scale_range[0]))        # define the 26 possible neighbors in 3D grid (3x3x3 cube minus the center point)    offsets = [        (dx, dy, dz)         for dx in [-dR_m, 0, +dR_m]         for dy in [+dE_L, 0, -dE_L]         for dz in [-dw_scale, 0, +dw_scale]        if not (dx == 0 and dy == 0 and dz == 0)]        neighbor_keys = []    for dx, dy, dz in offsets:        neighbor_R_m = min(R_m_range, key=lambda i: abs(i - R_m + dx)) # get closest value of grid to get rid of numerical rounding issues        neighbor_E_L = min(E_L_range, key=lambda i: abs(i - E_L + dy)) # get closest value of grid to get rid of numerical rounding issues        neighbor_w_scale = min(w_scale_range, key=lambda i: abs(i - w_scale + dz)) # get closest value of grid to get rid of numerical rounding issues        neighbor_key = f"Rm_{neighbor_R_m}_EL_{neighbor_E_L}_wscale_{neighbor_w_scale}"        if neighbor_key in results:            neighbor_keys.append(neighbor_key)        return neighbor_keysdef predict_trajectory_lowerE_maxOSI(results, coordinate):    # predict a trajectory with lowers the total energy and among lower energy nerighbors maximizes the OSI        # input    # results is the dictionary of the simulation data    # coordinate is a tuple of the (R_m, E_L, w_scale) coordinate    # output    # trajectory is a list of coordinate tuples    # get closest grid points to initial coordinate    closest_grid_point_coordinate, closest_key = find_closest_grid_point_to_coordinate(results, coordinate)    # initialize trajectory with closest grid point    current_key = closest_key    current_coordinate = closest_grid_point_coordinate    trajectory = [current_coordinate]        while True:        current_E_tot = results[current_key]['E_tot']        current_OSI = results[current_key]['OSI']                print(f"current location: {current_coordinate[0]} MOhm, {current_coordinate[1]} mV, {current_coordinate[2]}, E_tot: {current_E_tot:.1e}, OSI: {current_OSI:.2f}")        # find neighbor keys with lower energy consumption        neighbor_keys = find_neighbors(results, current_coordinate)                # filter neighbor keys with lower energy consumption and OSI > 0        lower_energy_neighbor_keys = [key for key in neighbor_keys if results[key]['E_tot'] < current_E_tot and results[key]['OSI'] > 0]                if not lower_energy_neighbor_keys:            break                # find the neighbor with the highest OSI        next_key = max(lower_energy_neighbor_keys, key=lambda k: results[k]['OSI'])                # update current position/coordinate        next_R_m, next_E_L, next_w_scale = map(float, [next_key.split('_')[1], next_key.split('_')[3], next_key.split('_')[5]])                next_coordinate = (next_R_m, next_E_L, next_w_scale)        trajectory.append(next_coordinate) # append coordinate        # update current position        current_key = next_key        current_coordinate = next_coordinate            #next_E_tot = results[next_key]['E_tot']        #next_OSI = results[next_key]['OSI']        return trajectorydef predict_all_trajectories(results):    # predict all trajectories/trajectories starting from all grid points        # input    # results is the dictionary of the simulation data    # output    # all_trajectories is a list of list of tuples of the coordinates        # initialize list to store all trajectories    all_trajectories = []        # iterate over all starting points in the grid    for key in results:        params = key.split('_')        R_m = float(params[1])        E_L = float(params[3])        w_scale = float(params[5])        coordinate = (R_m, E_L, w_scale)                OSI_value = results[key].get("OSI", 0)                  if OSI_value > 0:  # only calculate trajectory if OSI > 0, filters out all non-spikers            trajectory = predict_trajectory_lowerE_maxOSI(results, coordinate)            all_trajectories.append(trajectory)        return all_trajectories################################ Padamsey data analysis functions ################################def clean_voltage_trace(signal, thresholds, fs=1000, window=200, filter_quality=0.1, m=50):    # clean voltage trace by removing threshold-based anomalies, interpolating missing segments, removing oscillatory noise via a notch filter, and subtracting a moving average baseline    # input    # signal is a list or array of the voltage trace to be cleaned    # thresholds is a list of upper and lower threshold voltages, below & above these limits are considered anomalies and removed    # fs is the sampling frequency in Hz    # window is the size of samples to remove before and after an anomalous point    # filter_quality is the Q-factor of the notch filter used to remove the dominant spectral peak    # m is the window size for the moving average baseline subtraction    # output    # first_sig is an array of the original signal with its global mean (over non-NaN samples) subtracted    # final_sig is an array of the fully cleaned signal:        #- anomalies removed (set to NaN),        #- NaNs internally interpolated for filtering,        #- oscillations removed via notch filter,        #- moving-average baseline removed,        #- NaNs restored at anomaly locations        # remove anomalies    sig = np.asarray(signal)    n = sig.size    # find all peak indices    peak_idxs = np.where(sig > thresholds[1])[0]    trough_idxs = np.where(sig < thresholds[0])[0]    # initialize mask    mask = np.ones(n, dtype=bool) # True means keep    # for each peak, mask out window around it    for idx in peak_idxs:        start = max(idx - window, 0)        end   = min(idx + window + 1, n)        mask[start:end] = False    for idx in trough_idxs:        start = max(idx - window, 0)        end   = min(idx + window + 1, n)        mask[start:end] = False    # build cleaned signal    cleaned = sig.copy().astype(float)    cleaned[~mask] = np.nan       # find first spectral peak, interpolate over NaNs    idx = np.arange(n)    valid = ~np.isnan(cleaned)    if not np.all(valid):        # at least one NaN: fill via linear interp over index        cleaned = np.interp(idx, idx[valid], cleaned[valid])  # :contentReference[oaicite:2]{index=2}    # compute PSD using Welch's method, find lowest frequency oscillation    f, Pxx = welch(sig, fs=fs, window='hann', nperseg=None, noverlap=None, scaling='density')      first_spectral_peak = f[np.argmax(Pxx)]    # remove oscillations    nans = np.isnan(cleaned)    xp = np.flatnonzero(~nans)    fp = cleaned[~nans]    x  = np.flatnonzero(nans)    sig_interp = cleaned.copy()    sig_interp[nans] = np.interp(x, xp, fp)    b, a = iirnotch(first_spectral_peak , Q=filter_quality, fs=fs)    cleaned_OUT = filtfilt(b, a, sig_interp)    cleaned_OUT[nans]=np.nan        # subtract moving averages    kernel = np.ones(m) / m    pad_width = m // 2    clean_padded = np.pad(cleaned_OUT, (pad_width, pad_width), mode='reflect')    mv_baseline = np.convolve(clean_padded, kernel, mode='valid')    st_baseline = np.nanmean(cleaned_OUT)    final_sig = cleaned_OUT - mv_baseline[1:]    final_sig[~mask] = np.nan    first_sig = signal-st_baseline        return first_sig , final_sig################################ functions to update old grid runs ################################def OLD_update_E_tot(results):    # update E_tot and respective quantities for all keys in the results dictionary         # input    # results is the dicitonary of the meaned 3D grid         # output    # results_updated_energy is the dictonary of the meaned 3D grid with updated energy calculation    results_updated_energy = cp.deepcopy(results) # make a deep copy of the result dictionary, simple editing edits the original dictionary        for key, value in results_updated_energy.items():                # split the key string to extract R_m, E_L, and w_scale        try:            key_parts = key.split('_')            R_m = float(key_parts[1])  # extract R_m            E_L = float(key_parts[3])  # extract E_L            # retrieve r_post and I_syn_e from the dictionary value            r_post = value['r_post']            I_syn_e = value['I_syn_e']            E_tot_old = value['E_tot']                        C_m = 100 # in pF            V_thresh = -50 # spike generation threshold in mV                        # recalculate E_tot            E_tot_new, *_ = E_tot_eq(R_m, C_m, E_L, r_post, I_syn_e, V_thresh)            value['E_tot'] = E_tot_new # update the dictionary with the new E_tot                        # recalculate efficiency measures            OSI = value['OSI']            OSI_per_energy_updated = OSI/E_tot_new*10**9            value['OSI_per_energy'] = OSI_per_energy_updated # update the dictionary with the new OSI_per_energy_updated            MI = value['MI']            MI_per_energy_updated = MI/E_tot_new*10**9            value['MI_per_energy'] = MI_per_energy_updated # update the dictionary with the new MI_per_energy_updated            TE = value['TE']            TE_per_energy_updated = TE/E_tot_new*10**9            value['TE_per_energy'] = TE_per_energy_updated # update the dictionary with the new TE_per_energy_updated            CV_ISI = value['CV_ISI']            CV_ISI_per_energy_updated = CV_ISI/E_tot_new*10**9            value['CV_ISI_per_energy'] = CV_ISI_per_energy_updated # update the dictionary with the new CV_ISI_per_energy_updated            MICE = value['MICE']            MICE_per_energy_updated = MICE/E_tot_new*10**9            value['MICE_per_energy'] = MICE_per_energy_updated # update the dictionary with the new MICE_per_energy_updated            TECE = value['TECE']            TECE_per_energy_updated = TECE/E_tot_new*10**9            value['TECE_per_energy'] = TECE_per_energy_updated # update the dictionary with the new TECE_per_energy_updated                    except Exception as e:            print(f"Error processing key {key}: {e}")        return results_updated_energy################################ OLD unused functions ################################def E_AP_eq_c_m(r_post, A_overlap_AP, c_m, S_m, Delta_V):    # input     # r_post is the postsynaptic spiking frequency in Hz    # A_overlap_AP is the overlap factor between sodium and potassium currents    # c_m is the specfic membrane capacitance in F/m^2    # S_m is the neuronal cell surface in m^2    # Delta_V is the voltage difference between threshold voltage and action potential peak voltage in V        # output    # E_AP is the action potential energy in ATP/s        e = 1.602e-19 # in C    E_AP = r_post * A_overlap_AP * S_m * c_m * Delta_V / (3 * e)    return E_APdef E_RP_eq_AL(V_Na, V_K, R_m, V_RP):    # input     # V_Na is the sodium reversal potential in V    # V_K is the potassium reversal potential in V    # R_m is the membrane resistance in Ohm    # V_RP is the resting potential in V        # output    # E_RP is the energy needed to keep the resting potential in ATP/s        e = 1.602e-19 # in C    E_RP = (V_Na - V_RP)*(V_RP - V_K)/(e * R_m * (V_RP + 2*V_Na - 3*V_K))    return E_RPdef E_ST_eq_experimental(f_pre_spike, N_boutons, E_postsyn):    p_ves = 0.6*np.exp(-f_pre_spike/10.6) + 0.128    return f_pre_spike * N_boutons * p_ves * E_postsyn# analysis functions Zeldenrustdef entropy(train):    # calculate entropy of a discrete variable    # input    # train is a discrete array of categorical values (e.g. hidden state binned into 0 and 1)    # output    # H is the entropy of the variable in bits        prob = individual_probability(train)    H = 0    for p in prob.values():        if p > 0:            H += -p * np.log2(p)    return Hdef conditional_entropy(hidden_state, train):    # calculate conditional entropy H(hidden_state | train)    # input    # hidden_state is a discrete array with the hidden state (e.g. binary 0 and 1)    # train is a discrete array with the signal to condition on (e.g. discretized g_e or binned spike train)    # output    # H_xy is the conditional entropy of hidden state given the signal in bits    assert len(hidden_state) == len(train)    total_count = len(hidden_state)    p_y_dict = individual_probability(train)    joint_counts = Counter(zip(train, hidden_state))    H_xy = 0    for y_val, p_y in p_y_dict.items():        count_x1 = joint_counts.get((y_val, 1), 0)        count_x0 = joint_counts.get((y_val, 0), 0)        total_y = count_x1 + count_x0        if total_y == 0:            continue        p_x1_given_y = count_x1 / total_y        p_x0_given_y = count_x0 / total_y        entropy_y = 0        if p_x1_given_y > 0:            entropy_y += -p_x1_given_y * np.log2(p_x1_given_y)        if p_x0_given_y > 0:            entropy_y += -p_x0_given_y * np.log2(p_x0_given_y)        H_xy += p_y * entropy_y    return H_xydef time_bin_binary_rate_bin_hidden_state_conductance_spike_train_Zeldenrust(t_stim, g_e, spike_times_post, bin_time_mi, bins_conductance, T):    # bin hidden state input (binary), excitatory conductance & postsynaptic spike train (binary) in time        # input    # t_stim is the hidden state array with high temporal resolution    # g_e is an array of the the excitatory conductances in nS     # spike_times_post is a list of postsynaptic spike times in ms    # bin_time is the binsize in time in ms    # bins_conductance is the number of bins for the conductance    # g_e_precision is the number of decimals used for discretizing the g_e input    # T is the duration of the simulation in ms        # output    # hidden_state_binned is an array of the time binned hidden state    # g_e_discreted is an array of the time binned and discretized conductance    # spike_times_post_binned is an array of the time binned and binarized spike_times_post        n_bins = int(T / bin_time_mi)        # bin hidden state by interpolating it & accounting for borders    hidden_state_binned = np.interp(np.linspace(0, T, n_bins), np.linspace(0, T, len(t_stim)), t_stim)    hidden_state_binned = (hidden_state_binned > 0.5).astype(int)  # binary 0 or 1    # bin g_e    # bin g_e in time    g_e_binned = np.interp(np.linspace(0, T, n_bins), np.linspace(0, T, len(g_e)), g_e)    # discretize g_e using fixed number of bins    bin_edges = np.linspace(np.min(g_e_binned), np.max(g_e_binned), int(bins_conductance) + 1)    g_e_discreted = np.digitize(g_e_binned, bin_edges) - 1    g_e_discreted = np.clip(g_e_discreted, 0, bins_conductance - 1)    # bin g_e using dt    #g_e_binned = np.interp(np.linspace(0, T, n_bins), np.linspace(0, T, len(g_e)), g_e)    # discretize g_e by rounding (turn continuous into categorical)    #g_e_discreted = np.round(g_e_binned, decimals=g_e_precision)         # bin output spike train    spike_times_post_binned = np.zeros(n_bins)    spike_indices = (spike_times_post / bin_time_mi).astype(int)    spike_indices = spike_indices[spike_indices < n_bins]    spike_times_post_binned[spike_indices] = 1 # it only matters if neuron has spiked or not & not the rate?     return hidden_state_binned, g_e_discreted, spike_times_post_binneddef calculate_mutual_information_experimental_data_Zeldenrust(hidden_state, I_inj, spike_times_post, T, neuron_type_mode='inhibitory'):    # calculate mutual information of experimental Zeldenrust data        # input    # hidden_state is the hidden state array with high temporal resolution    # I_inj is an array of injected current in pA    # spike_times_post is a list of postsynaptic spike times in ms    # T is the duration of the simulation in ms    # neuron_type_mode is the assumed type of the neuron        # output    # MI_I is the mutual information between hidden state and input in bits    # MI_spike_train is the mutual information between hidden state and output spike train in bits    # H_xx is the entropy of the hidden state in bits    # H_xy_input is the conditional entropy of hidden state given input in bits    # H_xy_output is the conditional entropy of hidden state given output spike train in bits    # fraction_transferred_entropy is the fraction of hidden state entropy transferred to spikes    # fraction_transferred_information is the fraction of input information transferred to spikes        bin_time_mi = 50 # in ms    bins_conductance = 100        if neuron_type_mode == 'excitatory':        bin_time_mi = 250 # in ms        bins_conductance = 100        if neuron_type_mode == 'inhibitory':        bin_time_mi = 50 # in ms        bins_conductance = 100            if neuron_type_mode == 'Dopamine':        bin_time_mi = 200 # in ms        bins_conductance = 100    # downsample experimental data    #downsample_factor = int(sampling_rate/10)    #hidden_state_ds = hidden_state[::downsample_factor] # downsample hidden state to 0.1 ms    #I_inj_ds = I_inj[::downsample_factor] # downsample injected current to 0.1 ms    hidden_state_binned, g_e_discreted, spike_times_post_binned = time_bin_binary_rate_bin_hidden_state_conductance_spike_train_Zeldenrust(hidden_state, I_inj, spike_times_post, bin_time_mi, bins_conductance, T)    MI_I, MI_spike_train, MI_I_spike_train, H_xx, H_xy_input, H_xy_output, fraction_transferred_entropy, fraction_transferred_information = mutual_information_analysis_full_Zeldenrust(hidden_state_binned, g_e_discreted, spike_times_post_binned)    return MI_I, MI_spike_train, MI_I_spike_train, H_xx, H_xy_input, H_xy_output, fraction_transferred_entropy, fraction_transferred_informationdef mutual_information_analysis_full_Zeldenrust(hidden_state_binned, g_e_discreted, spike_times_post_binned):    # calculate mutual information analysis using dt-based binning    # input    # hidden_state_binned is an array of the time binned hidden state    # g_e_discreted is an array of the time binned and discretized conductance    # spike_times_post_binned is an array of the time binned and binarized spike_times_post        # output (printed results)    # MI_I is the mutual information between hidden state and input in bits    # MI_spike_train is the mutual information between hidden state and output spike train in bits    # H_xx is the entropy of the hidden state in bits    # H_xy_input is the conditional entropy of hidden state given input in bits    # H_xy_output is the conditional entropy of hidden state given output spike train in bits    # fraction_transferred_entropy is the fraction of hidden state entropy transferred to spikes    # fraction_transferred_information is the fraction of input information transferred to spikes            # mutual information (MI_I) between hidden state & convoled weighted input (g_e)    MI_I = mutual_information(hidden_state_binned, g_e_discreted)        # mutual information between hidden state & output neuronal response (spike_times_post)    MI_spike_train = mutual_information(hidden_state_binned, spike_times_post_binned)    MI_I_spike_train = mutual_information(g_e_discreted, spike_times_post_binned)        # entropy of hidden state    H_xx = entropy(hidden_state_binned)        # fractions    fraction_transferred_entropy = MI_spike_train / H_xx    fraction_transferred_information = MI_spike_train / MI_I    #print(f"Hidden state entropy: {H_xx:.3f} bits")    #print(f"MI_I (hidden state → input): {MI_I:.3f} bits")    #print(f"MI_spike_train (hidden state → output spikes): {MI_spike_train:.3f} bits")    #print(f"MI_I_spike_train (input → output spikes): {MI_I_spike_train:.3f} bits")        # H_xx > MI_I > MI_spike_train    # conditional entropies    H_xy_input = conditional_entropy(hidden_state_binned, g_e_discreted)    H_xy_output = conditional_entropy(hidden_state_binned, spike_times_post_binned)    # MI from conditional entropies (validation)    #MI_I_test = H_xx - H_xy_input    #MI_spike_train_test = H_xx - H_xy_output        #print(f"\nTest:")    #print(f"Conditional entropy (hidden state → input): {H_xy_input:.3f} bits")    #print(f"MI_I (hidden state → input): {MI_I_test:.3f} bits")    #print(f"Conditional entropy (hidden state → output spikes): {H_xy_output:.3f} bits")    #print(f"MI_spike_train (hidden state → output spikes): {MI_spike_train_test:.3f} bits")    #print(f"\nFractions:")    #print(f"Fraction transferred entropy: {fraction_transferred_entropy:.3f}")    #print(f"Fraction transferred information: {fraction_transferred_information:.3f}")    return MI_I, MI_spike_train, MI_I_spike_train, H_xx, H_xy_input, H_xy_output, fraction_transferred_entropy, fraction_transferred_informationdef hit_rate(spike_times_post, hidden_state, dt):     #     # inout    # spike_times_post is an array of spiketimes in ms    # hidden_state is an array of the hidden state in dt    # dt is the simulation time step dt in ms        # output    # hit_fraction is the fraction of spikes during ON state    # false_alarm_fraction is the fraction of spikes during OFF state        # convert spike times to indices    spike_indices = np.floor(spike_times_post / dt).astype(int)        # keep only spikes within simulation range    valid = spike_indices < len(hidden_state)    spike_indices = spike_indices[valid]        # hidden state at spike times    state_at_spikes = hidden_state[spike_indices]        hits = np.sum(state_at_spikes > 0.5)    false_alarms = np.sum(state_at_spikes <= 0.5)    total_spikes = hits + false_alarms        hit_fraction = hits / total_spikes    false_alarm_fraction = false_alarms / total_spikes        return hit_fraction, false_alarm_fractiondef complete_analysis_Zeldenrust(spike_times_post, V_m, T, R_m, C_m, E_L, I_syn_e, V_thresh, t_stim, g_e, neuron_type_mode, dt=1.0):    # perform complete analysis        # input    # spike_times_post is a list of postsynaptic spike times    # V_m is the time-dependent subthreshold membrane voltage in mV    # T is the duration of the simulation in ms    # R_m is the membrane resistance in MOhm    # C_m is the membrane capacitance in pF    # E_L is the resting potential (leak reversal potential) in mV    # I_syn_e is an array of length n of excitatory synaptic current    # V_thresh is the threshold voltage in mV    # t_stim is the hidden state array with high temporal resolution    # g_e is an array of the the excitatory conductances in nS     # neuron_type_mode is a string ('excitatory' or 'inhibitory') determining the switching parameters    # dt is the simulation time step dt in ms        # output    # r_post is the postsynaptic firing rate in Hz    # E_tot is the total energy in ATP/s    # CV_V_m is the CV of the subthreshold membrane voltage normed by distance to threshold´    # CV_ISI is the CVs based on ISIs    # CV_ISI_per_energy CV_ISI is the CVs based on ISIs per energy in 1/(10^9 ATP/s)    # MI is the mutual information between stimulus and neuronal response in bits    # MI_per_energy is the mutual information between stimulus and neuronal response per energy in bits/(10^9 ATP/s)    # MICE is the mutual information per spike in bits    # MICE_per_energy is the mutual information per spike per energy in bits/(10^9 ATP/s)    # MI_vec is a tuple containing different MI measures    # E_vec is a tuple containing the total energy and all energy contributions    # hit_fraction is the fraction of spikes during ON state    # false_alarm_fraction is the fraction of spikes during OFF state            # calculate CV and MI (per energy)    if len(spike_times_post) < 3:        # calculate energy and firing frequency         r_post = 0 # None        E_tot, E_HK, E_RP, E_AP, E_ST, E_glu, E_Ca = E_tot_eq(R_m, C_m, E_L, r_post, I_syn_e, V_thresh)        E_vec = (E_tot, E_HK, E_RP, E_AP, E_ST, E_glu, E_Ca)                # variability of the substhreshold membrane voltage        CV_V_m = calculate_CV_V_m(V_m, V_thresh)        CV_ISI, CV_ISI_per_energy, MI, MI_per_energy, MICE, MICE_per_energy = 0, 0, 0, 0, 0, 0 # None, None, None, None, None, None, None, None, None, None, None, None        MI_vec = (0, 0, 0, 0, 0, 0, 0, 0)        hit_fraction, false_alarm_fraction = 0, 0            if len(spike_times_post) >= 3:        # calculate energy and firing frequency         r_post = len(spike_times_post) / T * 1000 # is the postsynaptic firing rate in Hz        E_tot, E_HK, E_RP, E_AP, E_ST, E_glu, E_Ca = E_tot_eq(R_m, C_m, E_L, r_post, I_syn_e, V_thresh)        E_vec = (E_tot, E_HK, E_RP, E_AP, E_ST, E_glu, E_Ca)            # variability of the substhreshold membrane voltage        CV_V_m = calculate_CV_V_m(V_m, V_thresh)        # calculate coefficient of variation        CV_ISI = calculate_CV_ISI(spike_times_post)        CV_ISI_per_energy = CV_ISI / E_tot *10**9                # calculate mututal information        if neuron_type_mode == 'excitatory':            bin_time_mi = 250 # in ms            bins_conductance = 100                if neuron_type_mode == 'inhibitory':            bin_time_mi = 50 # in ms            bins_conductance = 100                hidden_state_binned, g_e_discreted, spike_times_post_binned = time_bin_binary_rate_bin_hidden_state_conductance_spike_train_Zeldenrust(t_stim, g_e, spike_times_post, bin_time_mi, bins_conductance, T)        MI_I, MI_spike_train, MI_I_spike_train, H_xx, H_xy_input, H_xy_output, fraction_transferred_entropy, fraction_transferred_information = mutual_information_analysis_full_Zeldenrust(hidden_state_binned, g_e_discreted, spike_times_post_binned)        MI_vec = (MI_I, MI_spike_train, MI_I_spike_train, H_xx, H_xy_input, H_xy_output, fraction_transferred_entropy, fraction_transferred_information)        MI = MI_spike_train        MI_per_energy = MI/E_tot*10**9        MICE = MI / r_post        MICE_per_energy = MI_per_energy / r_post        hit_fraction, false_alarm_fraction = hit_rate(spike_times_post, t_stim, dt)            return r_post, E_tot, CV_V_m, CV_ISI, CV_ISI_per_energy, MI, MI_per_energy, MICE, MICE_per_energy, MI_vec, E_vec, hit_fraction, false_alarm_fraction def get_optimal_bin_size_Zeldenrust(t_stim, g_e, spike_times_post, bin_time_mi_range, T):     # test different bin sizes (time)        # input    # t_stim is the hidden state array with high temporal resolution    # g_e is an array of the the excitatory conductances in nS     # spike_times_post is a list of postsynaptic spike times in ms    # bin_time_mi_range is an array of binsizes in time in ms    # T is the duration of the simulation in ms    # output     # MI_spike_train_values is an array of different binning results        MI_spike_train_values = []        # loop through dt values    for bin_time_mi in bin_time_mi_range:        # run mutual information analysis        hidden_state_binned, g_e_discreted, spike_times_post_binned = time_bin_binary_rate_bin_hidden_state_conductance_spike_train_Zeldenrust(t_stim, g_e, spike_times_post, bin_time_mi, bins_conductance=100, T=T)        MI_I, MI_spike_train, MI_I_spike_train, H_xx, H_xy_input, H_xy_output, fraction_transferred_entropy, fraction_transferred_information = mutual_information_analysis_full_Zeldenrust(hidden_state_binned, g_e_discreted, spike_times_post_binned)            # store MI_spike_train        MI_spike_train_values.append(MI_spike_train)    # convert to numpy array    MI_spike_train_values = np.array(MI_spike_train_values)    return MI_spike_train_valuesdef get_optimal_bin_sizes_Zeldenrust(t_stim, g_e, spike_times_post, bin_time_mi_range, bins_conductance_range, T):     # test different bin sizes (time & rate space)        # input    # t_stim is the hidden state array with high temporal resolution    # g_e is an array of the the excitatory conductances in nS     # spike_times_post is a list of postsynaptic spike times in ms    # bin_time_mi_range is an array of binsizes in time in ms    # bins_conductance_range is an array of binsizes in conductance space    # T is the duration of the simulation in ms    # output    # binning_results is a list of different binning results        binning_results = [] # save results in list    # 2D grid search for optimal time and rate bin sizes/numbers    for bin_time_mi in bin_time_mi_range:        for bins_conductance in bins_conductance_range:            # get binned trains            hidden_state_binned, g_e_discreted, spike_times_post_binned = time_bin_binary_rate_bin_hidden_state_conductance_spike_train_Zeldenrust(t_stim, g_e, spike_times_post, bin_time_mi, bins_conductance, T)            # calculate MI for bin size/number combination            MI_I, MI_spike_train, MI_I_spike_train, H_xx, H_xy_input, H_xy_output, fraction_transferred_entropy, fraction_transferred_information = mutual_information_analysis_full_Zeldenrust(hidden_state_binned, g_e_discreted, spike_times_post_binned)            binning_results.append((int(bin_time_mi), int(bins_conductance), MI_spike_train, MI_I, MI_I_spike_train)) # save results    return binning_results  